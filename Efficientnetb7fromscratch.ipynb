{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Efficientnetb7fromscratch1.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZE22y3LsXpd"
      },
      "source": [
        "# Finetuning of ImageNet pretrained EfficientNet-B0 on CIFAR-100\n",
        "\n",
        "In 2019, new ConvNets architectures have been proposed in [\"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\"](https://arxiv.org/pdf/1905.11946.pdf) paper. According to the paper, model's compound scaling starting from a 'good' baseline provides an network that achieves  state-of-the-art on  ImageNet,  while  being 8.4x  smaller and 6.1x faster on inference than the best existing ConvNet.\n",
        "\n",
        "![efficientnets](https://github.com/abdulelahsm/ignite/blob/update-tutorials/examples/notebooks/assets/efficientnets.png?raw=1)\n",
        "\n",
        "Following the paper, EfficientNet-B0 model pretrained on ImageNet and finetuned on CIFAR100 dataset gives 88% test accuracy. Let's reproduce this result with Ignite. [Official implementation](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet) of EfficientNet uses Tensorflow, \n",
        "for our case we will borrow the code from [katsura-jp/efficientnet-pytorch](https://github.com/katsura-jp/efficientnet-pytorch), \n",
        "[rwightman/pytorch-image-models](https://github.com/rwightman/pytorch-image-models) and [lukemelas/EfficientNet-PyTorch](https://github.com/lukemelas/EfficientNet-PyTorch/) repositories (kudos to authors!). We will download pretrained weights from [lukemelas/EfficientNet-PyTorch](https://github.com/lukemelas/EfficientNet-PyTorch/) repository.\n",
        "\n",
        "## Network architecture review\n",
        "The architecture of EfficientNet-B0 is the following:\n",
        "```\n",
        "1 - Stem    - Conv3x3|BN|Swish\n",
        "\n",
        "2 - Blocks  - MBConv1, k3x3 \n",
        "            - MBConv6, k3x3 repeated 2 times\n",
        "            - MBConv6, k5x5 repeated 2 times\n",
        "            - MBConv6, k3x3 repeated 3 times\n",
        "            - MBConv6, k5x5 repeated 3 times\n",
        "            - MBConv6, k5x5 repeated 4 times\n",
        "            - MBConv6, k3x3\n",
        "                            totally 16 blocks\n",
        "\n",
        "3 - Head    - Conv1x1|BN|Swish \n",
        "            - Pooling\n",
        "            - Dropout\n",
        "            - FC\n",
        "```\n",
        "\n",
        "where \n",
        "```\n",
        "Swish(x) = x * sigmoid(x)\n",
        "```\n",
        "and `MBConvX` stands for mobile inverted bottleneck convolution, X - denotes expansion ratio:\n",
        "``` \n",
        "MBConv1 : \n",
        "  -> DepthwiseConv|BN|Swish -> SqueezeExcitation -> Conv|BN\n",
        "\n",
        "MBConv6 : \n",
        "  -> Conv|BN|Swish -> DepthwiseConv|BN|Swish -> SqueezeExcitation -> Conv|BN\n",
        "\n",
        "MBConv6+IdentitySkip : \n",
        "  -.-> Conv|BN|Swish -> DepthwiseConv|BN|Swish -> SqueezeExcitation -> Conv|BN-(+)->\n",
        "   \\___________________________________________________________________________/\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP_tseP1sXpl"
      },
      "source": [
        "## Installations\n",
        "\n",
        "1) Torchvision\n",
        "\n",
        "Please install torchvision in order to get CIFAR100 dataset: \n",
        "```\n",
        "conda install -y torchvision -c pytorch\n",
        "```\n",
        "\n",
        "2) Let's install Nvidia/Apex package:\n",
        "\n",
        "We will train with automatic mixed precision using [nvidia/apex](https://github.com/NVIDIA/apex) pacakge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "br990PJfgCHz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4808f0a-a551-4b56-a3a3-b1c011759204"
      },
      "source": [
        "# Install Apex:\n",
        "!pip install --upgrade --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" git+https://github.com/NVIDIA/apex/"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Collecting git+https://github.com/NVIDIA/apex/\n",
            "  Cloning https://github.com/NVIDIA/apex/ to /tmp/pip-req-build-qx1u6roj\n",
            "  Running command git clone -q https://github.com/NVIDIA/apex/ /tmp/pip-req-build-qx1u6roj\n",
            "  Running command git submodule update --init --recursive -q\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-qx1u6roj/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-qx1u6roj/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-oz1oyja2/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "naLdioPTgCH2"
      },
      "source": [
        "3) Install tensorboardX and `pytorch-ignite`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "watnTwx-sXpm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48d09a39-1f00-4edc-83c5-57b3c3b7f603"
      },
      "source": [
        "!pip install pytorch-ignite tensorboardX"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.7/dist-packages (0.4.4)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (2.2)\n",
            "Requirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (57.0.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FudN7dJBsXpp"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "import ignite\n",
        "\n",
        "seed = 17\n",
        "random.seed(seed)\n",
        "_ = torch.manual_seed(seed)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnytGeC5sXpq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1489c6d6-1685-48d3-8950-a21724b2dd51"
      },
      "source": [
        "torch.__version__, ignite.__version__"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1.8.1+cu101', '0.4.4')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZEFHfBwsXpr"
      },
      "source": [
        "## Model\n",
        "\n",
        "\n",
        "Let's define some helpful modules:\n",
        "- Flatten \n",
        "- Swish \n",
        "\n",
        "The reason why Swish is not implemented in `torch.nn` can be found [here](https://github.com/pytorch/pytorch/pull/3182).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rREuFNq1sXps"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x.reshape(x.shape[0], -1)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlOJJKnVsXpt"
      },
      "source": [
        "Let's visualize Swish transform vs ReLU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiQ5NmqasXpu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "105f7ce3-dbc5-48bb-eeb6-0f1ac9f2e4e6"
      },
      "source": [
        "import matplotlib.pylab as plt\n",
        "%matplotlib inline\n",
        "\n",
        "d = torch.linspace(-10.0, 10.0)\n",
        "s = Swish()\n",
        "res = s(d)\n",
        "res2 = torch.relu(d)\n",
        "\n",
        "plt.title(\"Swish transformation\")\n",
        "plt.plot(d.numpy(), res.numpy(), label='Swish')\n",
        "plt.plot(d.numpy(), res2.numpy(), label='ReLU')\n",
        "plt.legend()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: Not providing a value for linspace's steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at  /pytorch/aten/src/ATen/native/RangeFactories.cpp:23.)\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f04258ce5d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6Sp-82PsXpv"
      },
      "source": [
        "Now let's define `SqueezeExcitation` module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmvJRZQosXpw"
      },
      "source": [
        "class SqueezeExcitation(nn.Module):\n",
        "    \n",
        "    def __init__(self, inplanes, se_planes):\n",
        "        super(SqueezeExcitation, self).__init__()\n",
        "        self.reduce_expand = nn.Sequential(\n",
        "            nn.Conv2d(inplanes, se_planes, \n",
        "                      kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            Swish(),\n",
        "            nn.Conv2d(se_planes, inplanes, \n",
        "                      kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_se = torch.mean(x, dim=(-2, -1), keepdim=True)\n",
        "        x_se = self.reduce_expand(x_se)\n",
        "        return x_se * x\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CAxubs8sXpw"
      },
      "source": [
        "Next, we can define `MBConv`.\n",
        "\n",
        "**Note on implementation**: in Tensorflow (and PyTorch ports) convolutions use `SAME` padding option which in PyTorch requires\n",
        "a specific padding computation and additional operation to apply. We will use built-in padding argument of the convolution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-AL5AYysXpw"
      },
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class MBConv(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, planes, kernel_size, stride, \n",
        "                 expand_rate=1.0, se_rate=0.25, \n",
        "                 drop_connect_rate=0.2):\n",
        "        super(MBConv, self).__init__()\n",
        "\n",
        "        expand_planes = int(inplanes * expand_rate)\n",
        "        se_planes = max(1, int(inplanes * se_rate))\n",
        "\n",
        "        self.expansion_conv = None        \n",
        "        if expand_rate > 1.0:\n",
        "            self.expansion_conv = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, expand_planes, \n",
        "                          kernel_size=1, stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(expand_planes, momentum=0.01, eps=1e-3),\n",
        "                Swish()\n",
        "            )\n",
        "            inplanes = expand_planes\n",
        "\n",
        "        self.depthwise_conv = nn.Sequential(\n",
        "            nn.Conv2d(inplanes, expand_planes,\n",
        "                      kernel_size=kernel_size, stride=stride, \n",
        "                      padding=kernel_size // 2, groups=expand_planes,\n",
        "                      bias=False),\n",
        "            nn.BatchNorm2d(expand_planes, momentum=0.01, eps=1e-3),\n",
        "            Swish()\n",
        "        )\n",
        "\n",
        "        self.squeeze_excitation = SqueezeExcitation(expand_planes, se_planes)\n",
        "        \n",
        "        self.project_conv = nn.Sequential(\n",
        "            nn.Conv2d(expand_planes, planes, \n",
        "                      kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(planes, momentum=0.01, eps=1e-3),\n",
        "        )\n",
        "\n",
        "        self.with_skip = stride == 1\n",
        "        self.drop_connect_rate = torch.tensor(drop_connect_rate, requires_grad=False)\n",
        "    \n",
        "    def _drop_connect(self, x):        \n",
        "        keep_prob = 1.0 - self.drop_connect_rate\n",
        "        drop_mask = torch.rand(x.shape[0], 1, 1, 1) + keep_prob\n",
        "        drop_mask = drop_mask.type_as(x)\n",
        "        drop_mask.floor_()\n",
        "        return drop_mask * x / keep_prob\n",
        "        \n",
        "    def forward(self, x):\n",
        "        z = x\n",
        "        if self.expansion_conv is not None:\n",
        "            x = self.expansion_conv(x)\n",
        "\n",
        "        x = self.depthwise_conv(x)\n",
        "        x = self.squeeze_excitation(x)\n",
        "        x = self.project_conv(x)\n",
        "        \n",
        "        # Add identity skip\n",
        "        if x.shape == z.shape and self.with_skip:            \n",
        "            if self.training and self.drop_connect_rate is not None:\n",
        "                self._drop_connect(x)\n",
        "            x += z\n",
        "        return x"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlPTQlFRsXpx"
      },
      "source": [
        "And finally, we can implement generic `EfficientNet`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV2NfxZIsXpx"
      },
      "source": [
        "from collections import OrderedDict\n",
        "import math\n",
        "\n",
        "\n",
        "def init_weights(module):    \n",
        "    if isinstance(module, nn.Conv2d):    \n",
        "        nn.init.kaiming_normal_(module.weight, a=0, mode='fan_out')\n",
        "    elif isinstance(module, nn.Linear):\n",
        "        init_range = 1.0 / math.sqrt(module.weight.shape[1])\n",
        "        nn.init.uniform_(module.weight, a=-init_range, b=init_range)\n",
        "        \n",
        "        \n",
        "class EfficientNet(nn.Module):\n",
        "        \n",
        "    def _setup_repeats(self, num_repeats):\n",
        "        return int(math.ceil(self.depth_coefficient * num_repeats))\n",
        "    \n",
        "    def _setup_channels(self, num_channels):\n",
        "        num_channels *= self.width_coefficient\n",
        "        new_num_channels = math.floor(num_channels / self.divisor + 0.5) * self.divisor\n",
        "        new_num_channels = max(self.divisor, new_num_channels)\n",
        "        if new_num_channels < 0.9 * num_channels:\n",
        "            new_num_channels += self.divisor\n",
        "        return new_num_channels\n",
        "\n",
        "    def __init__(self, num_classes=100, \n",
        "                 width_coefficient=1.0,\n",
        "                 depth_coefficient=1.0,\n",
        "                 se_rate=0.25,\n",
        "                 dropout_rate=0.2,\n",
        "                 drop_connect_rate=0.2):\n",
        "        super(EfficientNet, self).__init__()\n",
        "        \n",
        "        self.width_coefficient = width_coefficient\n",
        "        self.depth_coefficient = depth_coefficient\n",
        "        self.divisor = 8\n",
        "                \n",
        "        list_channels = [32, 16, 24, 40, 80, 112, 192, 320, 1280]\n",
        "        list_channels = [self._setup_channels(c) for c in list_channels]\n",
        "                \n",
        "        list_num_repeats = [1, 2, 2, 3, 3, 4, 1]\n",
        "        list_num_repeats = [self._setup_repeats(r) for r in list_num_repeats]        \n",
        "        \n",
        "        expand_rates = [1, 6, 6, 6, 6, 6, 6]\n",
        "        strides = [1, 2, 2, 2, 1, 2, 1]\n",
        "        kernel_sizes = [3, 3, 5, 3, 5, 5, 3]\n",
        "\n",
        "        # Define stem:\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, list_channels[0], kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(list_channels[0], momentum=0.01, eps=1e-3),\n",
        "            Swish()\n",
        "        )\n",
        "        \n",
        "        # Define MBConv blocks\n",
        "        blocks = []\n",
        "        counter = 0\n",
        "        num_blocks = sum(list_num_repeats)\n",
        "        for idx in range(7):\n",
        "            \n",
        "            num_channels = list_channels[idx]\n",
        "            next_num_channels = list_channels[idx + 1]\n",
        "            num_repeats = list_num_repeats[idx]\n",
        "            expand_rate = expand_rates[idx]\n",
        "            kernel_size = kernel_sizes[idx]\n",
        "            stride = strides[idx]\n",
        "            drop_rate = drop_connect_rate * counter / num_blocks\n",
        "            \n",
        "            name = \"MBConv{}_{}\".format(expand_rate, counter)\n",
        "            blocks.append((\n",
        "                name,\n",
        "                MBConv(num_channels, next_num_channels, \n",
        "                       kernel_size=kernel_size, stride=stride, expand_rate=expand_rate, \n",
        "                       se_rate=se_rate, drop_connect_rate=drop_rate)\n",
        "            ))\n",
        "            counter += 1\n",
        "            for i in range(1, num_repeats):                \n",
        "                name = \"MBConv{}_{}\".format(expand_rate, counter)\n",
        "                drop_rate = drop_connect_rate * counter / num_blocks                \n",
        "                blocks.append((\n",
        "                    name,\n",
        "                    MBConv(next_num_channels, next_num_channels, \n",
        "                           kernel_size=kernel_size, stride=1, expand_rate=expand_rate, \n",
        "                           se_rate=se_rate, drop_connect_rate=drop_rate)                                    \n",
        "                ))\n",
        "                counter += 1\n",
        "        \n",
        "        self.blocks = nn.Sequential(OrderedDict(blocks))\n",
        "        \n",
        "        # Define head\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(list_channels[-2], list_channels[-1], \n",
        "                      kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(list_channels[-1], momentum=0.01, eps=1e-3),\n",
        "            Swish(),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            Flatten(),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(list_channels[-1], num_classes)\n",
        "        )\n",
        "\n",
        "        self.apply(init_weights)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        f = self.stem(x)\n",
        "        f = self.blocks(f)\n",
        "        y = self.head(f)\n",
        "        return y"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXcs9FAZQ45H",
        "outputId": "5d1dfdbe-6b60-4be3-d330-75d5fca840db"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMBVZexzRjlw"
      },
      "source": [
        "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaKAI_jtRnIY",
        "outputId": "f8bbea67-d9c1-462b-a595-79b0f3bbe799"
      },
      "source": [
        "print(device)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tEhbJWnsXpy"
      },
      "source": [
        "All EfficientNet models can be defined using the following parametrization:\n",
        "```\n",
        "# (width_coefficient, depth_coefficient, resolution, dropout_rate)\n",
        "'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n",
        "'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n",
        "'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n",
        "'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n",
        "'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n",
        "'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n",
        "'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n",
        "'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n",
        "```    \n",
        "Let's define and train the third one: `EfficientNet-B0`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCuCoTHBsXpy"
      },
      "source": [
        "model = EfficientNet(num_classes=1, \n",
        "                     width_coefficient=2.0, depth_coefficient=3.1, \n",
        "                     dropout_rate=0.5).to(device)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWP4xLuASTSw"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from math import ceil\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import  DataLoader\n",
        "import torchvision.datasets as datasetes\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQGul5TsSOn2"
      },
      "source": [
        "import os\n",
        "import  pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "#from skimage import io\n",
        "from PIL import Image"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efznYNr7SA2H"
      },
      "source": [
        "\n",
        "class cancer_data(Dataset):\n",
        "    def __init__(self,csv_file,root_dir,transform=None):\n",
        "        self.annot= pd.read_csv(csv_file)\n",
        "        self.root_dir=root_dir\n",
        "        self.transform=transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annot)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path =os.path.join(self.root_dir,self.annot.iloc[index , 0])\n",
        "        #print(self.annot.iloc[index ,0])\n",
        "        #print(self.annot.iloc[index, 1])\n",
        "        #image=io.imread(img_path)\n",
        "        image=Image.open(img_path)\n",
        "        image=image.resize((224,224))\n",
        "        y_label=torch.tensor(int(self.annot.iloc[index , 1]))\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return (image , y_label)\n",
        "        #return (torch.from_numpy(image),y_label)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbLIU7IlSes-"
      },
      "source": [
        "\n",
        "dataset_input=cancer_data(csv_file='/content/drive/MyDrive/rec.csv' , root_dir='/content/drive/MyDrive/img1/img' , transform=transforms.ToTensor())\n",
        "train_dataset=DataLoader(dataset=dataset_input ,batch_size=16 ,shuffle=True )"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTFLmacVShX8"
      },
      "source": [
        "criteria= nn.BCEWithLogitsLoss()\n",
        "optimizer=optim.Adam(model.parameters(),0.001)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix_v_X1zSnl7",
        "outputId": "c44ae579-fca3-442e-bfb0-b1adb991a58d"
      },
      "source": [
        "for ep in range(15):\n",
        "    tot_loss=0\n",
        "    for batch_idx,(data ,target) in enumerate(train_dataset):\n",
        "         #convert data of image into 2D to 1D\n",
        "         #print(data.shape)\n",
        "         #print(target)\n",
        "         #print(batch_idx)\n",
        "         if batch_idx%66==0:\n",
        "            print(\"=\",end=\" \")\n",
        "         #print(target)\n",
        "         #forward i.e pass obtain data to algorithm\n",
        "         data=data.to(device)\n",
        "         scores=model(data)\n",
        "         #print(scores)\n",
        "         target=target.to(device)\n",
        "         target = target.unsqueeze(1)\n",
        "         target =target.float()\n",
        "         loss=criteria(scores,target)\n",
        "         #print(loss)\n",
        "         tot_loss+=loss\n",
        "         optimizer.zero_grad()\n",
        "         loss.backward()\n",
        "         optimizer.step()\n",
        "    print(\"Epoch {}/{}\".format(ep + 1, 15))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "= = = = = = = = = = = Epoch 1/15\n",
            "= = = = = = = = = = = Epoch 2/15\n",
            "= = = = = = = = = = = Epoch 3/15\n",
            "= = = = = = = = = = = Epoch 4/15\n",
            "= = = = = = = = = = = Epoch 5/15\n",
            "= = = = = = = = = = = Epoch 6/15\n",
            "= = = = = = = = = = = Epoch 7/15\n",
            "= = = = = = = = = = = Epoch 8/15\n",
            "= = = = = = = = = = = Epoch 9/15\n",
            "= = = = = = = = = = = Epoch 10/15\n",
            "= = = = = = = = = = = Epoch 11/15\n",
            "= = = = = = = = = = = Epoch 12/15\n",
            "= = = = = = = = = = = Epoch 13/15\n",
            "= = = = = = = = = = = Epoch 14/15\n",
            "= = = = = = = = = = = Epoch 15/15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFBuYrPvpbaH"
      },
      "source": [
        "torch.save(model,'Efficientnetb0007.h5')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8BgQYFlGbiG"
      },
      "source": [
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DG_u-HC-D8B",
        "outputId": "19f3734e-4b4f-4bb1-903a-55220e5f8361"
      },
      "source": [
        "transform=transforms.ToTensor()\n",
        "model = torch.load('Efficientnetb0007.h5',map_location=torch.device('cuda'))\n",
        "model.eval()\n",
        "import  pandas as pd\n",
        "df=pd.read_csv('/content/drive/MyDrive/benign.csv')\n",
        "DF=df.iloc[1:,0:2]\n",
        "#print(DF)\n",
        "total=DF.count()[0]\n",
        "tp=tn=fp=fn=0\n",
        "for i in range(total):\n",
        "    if(i!=0 and i%100==0):\n",
        "        print(\"=\",end=\" \")\n",
        "    image = Image.open('/content/drive/MyDrive/benign1/benign/'+str(DF.iloc[i][0]))\n",
        "    image = image.resize((224, 224))\n",
        "    image = transform(image)\n",
        "    target=int(DF.iloc[i][1])\n",
        "    x = image.unsqueeze(0)\n",
        "    out = model(x.to(device))\n",
        "    n = float(out.data[0][0]) >= 0.5\n",
        "    if(n and target==1):\n",
        "        tp+=1\n",
        "    if(n and target==0):\n",
        "        fp+=1\n",
        "    if(not n and target==1):\n",
        "        fn+=1\n",
        "    if(not n and target==0):\n",
        "        tn+=1\n",
        "print(\"= = = = = = = = = =\")\n",
        "print(\"True Positive: \"+str(tp))\n",
        "print(\"True Negative: \"+str(tn))\n",
        "print(\"False Positive:\"+str(fp))\n",
        "print(\"False Negative:\"+str(fn))\n",
        "Total=tp+tn+fp+fn\n",
        "acc=((tp+tn)/Total)*100\n",
        "print(\"Accuracy :  \"+str(acc)+\"%\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "= = = = = = = = = = = = = = = = = = = = = =\n",
            "True Positive: 645\n",
            "True Negative: 408\n",
            "False Positive:0\n",
            "False Negative:197\n",
            "Accuracy :  84.24000000000001%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjUCNwu3sXpz"
      },
      "source": [
        "Number of parameters:"
      ]
    }
  ]
}